model:
  _target_ : scaling_recipes.model.MLP
  n_classes: 10
  width: 32
  n_blocks: 5
  input_mult: 1
  output_mult: 1
  parametrization: sp
  
trainer:
  max_epochs: 25
  loss: 
    _target_: scaling_recipes.loss.ClassificationLoss
  optimizer:
    _target_: torch.optim.AdamW 
    lr: 0.001           
    weight_decay: 0.001 
    lr_scheduler: linear_decay
    gradient_clip: 1.0

data:
  _target_: scaling_recipes.datasets.MNISTDataset 
  size: 50000
  batch_size: 10000

logger:
  _target_: scaling_recipes.logger.Logger
  log_interval_epochs: 1
  log_dir: logs/mnist/${model.parametrization}_${model.width}_lr${trainer.optimizer.lr}
  loss_dir: loss
  ckpt_dir: ckpt  

sweep:
  widths: [8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096]
  n_blocks: [5, 10, 20]
  lr_range: [-10, -3]
  lr_intervals: 25
  save_file: sweep_plots/sweep_metrics_${model.parametrization}_${model.width}_lr${trainer.optimizer.lr}.png