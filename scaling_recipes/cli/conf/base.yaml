model:
  _target_ : scaling_recipes.model.MLP
  n_classes: 10
  width: 32
  n_blocks: 1
  input_mult: 1
  output_mult: 1
  parametrization: mup
  
trainer:
  max_epochs: 25
  loss: 
    _target_: scaling_recipes.loss.ClassificationLoss
  optimizer:
    _target_: torch.optim.AdamW 
    lr: 0.001           # Learning rate
    weight_decay: 0.001  # Weight decay (L2 regularization)
    lr_scheduler: linear_decay
    gradient_clip: 1.0

data:
  _target_: scaling_recipes.datasets.MNISTDataset 
  size: 50000
  batch_size: 10000

logger:
  _target_: scaling_recipes.logger.Logger
  log_interval_epochs: 1
  log_dir: logs/mnist/${model.parametrization}_${model.width}_lr${trainer.optimizer.lr}
  loss_dir: loss
  ckpt_dir: ckpt  

sweep:
  widths: [8, 16, 32] #, 64, 128, 256, 512, 1024, 2048, 4096]
  n_blocks: [5, 10, 20]
  lr_range: [-10, -1]
  lr_intervals: 10
  save_file: sweep_plots/${model.parametrization}_${model.width}_lr${trainer.optimizer.lr}.png